{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #999900\">**SparkContexte ou SparkSession**</span>\n",
    "  \n",
    "<p>\n",
    "Depuis les versions antérieures de Spark ou Pyspark, SparkContext (JavaSparkContext pour Java) est un point d'entrée pour la programmation Spark avec RDD et pour se connecter à Spark Cluster, depuis Spark 2.0 SparkSession a été introduit et est devenu un point d'entrée pour commencer à programmer avec DataFrame et ensemble de données.\n",
    "<p>  \n",
    "\n",
    "\n",
    "<span style=\"color: #999900\">Qu'est-ce que SparkContext</span>\n",
    "\n",
    "Spark SparkContext est un point d'entrée pour Spark et défini dans le org.apache.sparkpackage depuis 1.x et utilisé pour créer par programme Spark RDD , des accumulateurs et des variables de diffusion sur le cluster.  \n",
    "Depuis Spark 2.0, la plupart des fonctionnalités (méthodes) disponibles dans SparkContext sont également disponibles dans SparkSession.  \n",
    "Son objet sc est disponible par défaut dans spark-shell et il peut être créé par programme à l'aide SparkContext de la classe.\n",
    "\n",
    "<span style=\"color: #999900\">Qu'est-ce que SparkSession</span>  \n",
    "<p>\n",
    "SparkSession introduit dans la version 2.0 et est un point d'entrée vers la fonctionnalité Spark sous-jacente afin de créer par programme Spark RDD, DataFrame et DataSet.  \n",
    "Son objet sparkest disponible par défaut dans spark-shell et il peut être créé par programme à l'aide du modèle de générateur SparkSession.  \n",
    "<p>\n",
    "\n",
    "<p>\n",
    "Avec Spark 2.0, une nouvelle classe org.apache.spark.sql.SparkSessiona été introduite, qui est une classe combinée pour tous les différents contextes que nous avions avant la version 2.0 (SQLContext et HiveContext, etc.).  \n",
    "Par conséquent, SparkSession peut être utilisé en remplacement de SQLContext et HiveContext.\n",
    "\n",
    "Comme mentionné au début, SparkSession est un point d'entrée vers Spark et la création d'une instance SparkSession serait la première instruction que vous écririez pour programmer avec RDD , DataFrame et Dataset et SparkSession seront créés à l'aide SparkSession.builder()\n",
    "\n",
    "Spark Session comprend également toutes les API disponibles dans différents contextes :\n",
    "- Spark Context,\n",
    "- SQL Context,\n",
    "- Streaming Context,\n",
    "- Hive Context.\n",
    "<p>\n",
    "\n",
    "**Exemple :**   \n",
    "sqlcontext = spark.sqlContext\n",
    "\n",
    "**Creating SparkSession**  \n",
    "spark = SparkSession.builder() \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .getOrCreate();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1975b92190f0b661a6d7a57dfdf52e325a2803f2b5af6696dc8b494e1877b0c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
