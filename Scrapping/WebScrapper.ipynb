{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gardez à l'esprit que la plupart des sites Web bloquent les requêtes qui ne contiennent pas d'en-tête  User-Agent valide.  \n",
    "L'en-tête User-Agent de la demande est une chaîne qui caractérise l'application et la version du système d'exploitation d'où provient la demande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "url = 'https://quotes.toscrape.com'\n",
    "page = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = []\n",
    "\n",
    "quote_elements = soup.find_all('div', class_='quote')\n",
    "\n",
    "for quote_element in quote_elements:\n",
    "\n",
    "    # extracting the text of the quote\n",
    "    text = quote_element.find('span', class_='text').text\n",
    "    \n",
    "    # extracting the author of the quote\n",
    "    author = quote_element.find('small', class_='author').text\n",
    "\n",
    "    # extracting the tag <a> HTML elements related to the quote\n",
    "    tag_elements = quote_element.find('div', class_='tags').find_all('a', class_='tag')\n",
    "\n",
    "    # storing the list of tag strings in a list\n",
    "    tags = []\n",
    "    for tag_element in tag_elements:\n",
    "        tags.append(tag_element.text)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grâce à la find() méthode de Beautiful Soup, vous pouvez extraire le seul élément HTML qui vous intéresse.  \n",
    "Étant donné que les balises associées au quote sont multiples, vous devez les stocker dans une liste. \n",
    "\n",
    "Ensuite, vous pouvez transformer ces données en dictionnaire et les ajouter à la quote liste comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes.append(\n",
    "    {\n",
    "        'text': text,\n",
    "        'author': author,\n",
    "        'tags': ', '.join(tags) # merging the tags into a \"A, B, ..., Z\" string\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '“A day without sunshine is like, you know, night.”', 'author': 'Steve Martin', 'tags': 'humor, obvious, simile'}]\n"
     ]
    }
   ],
   "source": [
    "print(quotes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En suivant le lien contenu dans l'élément HTML \"Suivant \"a\", vous pouvez facilement naviguer sur l'ensemble du site.  \n",
    "Alors, commençons par la page d'accueil et voyons comment parcourir chaque page qui compose le site Web cible.  \n",
    "Tout ce que vous avez à faire est de rechercher le next \"li\" élément HTML et d'extraire le lien relatif vers la page suivante.\n",
    "\n",
    "Vous pouvez implémenter la logique d'exploration comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the url of the home page of the target website\n",
    "base_url = 'https://quotes.toscrape.com'\n",
    "\n",
    "# retrieving the page and initializing soup...\n",
    "\n",
    "# getting the \"Next →\" HTML element\n",
    "next_li_element = soup.find('li', class_='next')\n",
    "\n",
    "# if there is a next page to scrape\n",
    "while next_li_element is not None:\n",
    "    next_page_relative_url = next_li_element.find('a', href=True)['href']\n",
    "\n",
    "    # getting the new page\n",
    "    page = requests.get(base_url + next_page_relative_url, headers=headers)\n",
    "\n",
    "    # parsing the new page\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # scraping logic...\n",
    "\n",
    "    # looking for the \"Next →\" HTML element in the new page\n",
    "    next_li_element = soup.find('li', class_='next')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion des données au format CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# scraping logic...\n",
    "\n",
    "# reading  the \"quotes.csv\" file and creating it\n",
    "# if not present\n",
    "csv_file = open('quotes.csv', 'w', encoding='utf-8', newline='')\n",
    "\n",
    "# initializing the writer object to insert data\n",
    "# in the CSV file\n",
    "writer = csv.writer(csv_file)\n",
    "\n",
    "# writing the header of the CSV file\n",
    "writer.writerow(['Text', 'Author', 'Tags'])\n",
    "\n",
    "# writing each row of the CSV\n",
    "for quote in quotes:\n",
    "    writer.writerow(quote.values())\n",
    "\n",
    "# terminating the operation and releasing the resources\n",
    "csv_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce que fait cet extrait est d'écrire les données de citation contenues dans la liste des dictionnaires dans un fichier quotes.csv .  \n",
    "Notez que cela fait partie de la bibliothèque standard Python. Ainsi, vous pouvez l'importer et l'utiliser sans installer de dépendance supplémentaire.  \n",
    "Dans le détail, il suffit de créer un fichier avec open().  \n",
    "Ensuite, vous pouvez le remplir avec la fonction writerow() de l'objet Writer de la bibliothèque csv.  \n",
    "Cela écrira chaque dictionnaire de citation sous la forme d'une ligne au format CSV dans le fichier CSV.\n",
    "\n",
    "Vous êtes passé de données brutes contenues dans un site Web à des données structurées stockées dans un fichier CSV.  \n",
    "Le processus d'extraction de données est terminé et vous pouvez maintenant jeter un œil à l'ensemble du scrapp Web Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOUT ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_page(soup, quotes):\n",
    "    # retrieving all the quote <div> HTML element on the page\n",
    "    quote_elements = soup.find_all('div', class_='quote')\n",
    "\n",
    "    # iterating over the list of quote elements\n",
    "    # to extract the data of interest and store it\n",
    "    # in quotes\n",
    "    for quote_element in quote_elements:\n",
    "        # extracting the text of the quote\n",
    "        text = quote_element.find('span', class_='text').text\n",
    "        # extracting the author of the quote\n",
    "        author = quote_element.find('small', class_='author').text\n",
    "\n",
    "        # extracting the tag <a> HTML elements related to the quote\n",
    "        tag_elements = quote_element.find('div', class_='tags').find_all('a', class_='tag')\n",
    "\n",
    "        # storing the list of tag strings in a list\n",
    "        tags = []\n",
    "        for tag_element in tag_elements:\n",
    "            tags.append(tag_element.text)\n",
    "\n",
    "        # appending a dictionary containing the quote data\n",
    "        # in a new format in the quote list\n",
    "        quotes.append(\n",
    "            {\n",
    "                'text': text,\n",
    "                'author': author,\n",
    "                'tags': ', '.join(tags)  # merging the tags into a \"A, B, ..., Z\" string\n",
    "            }\n",
    "        )\n",
    "\n",
    "# the url of the home page of the target website\n",
    "base_url = 'https://quotes.toscrape.com'\n",
    "\n",
    "# defining the User-Agent header to use in the GET request below\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# retrieving the target web page\n",
    "page = requests.get(base_url, headers=headers)\n",
    "\n",
    "# parsing the target web page with Beautiful Soup\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# initializing the variable that will contain\n",
    "# the list of all quote data\n",
    "quotes = []\n",
    "\n",
    "# scraping the home page\n",
    "scrape_page(soup, quotes)\n",
    "\n",
    "# getting the \"Next →\" HTML element\n",
    "next_li_element = soup.find('li', class_='next')\n",
    "\n",
    "# if there is a next page to scrape\n",
    "while next_li_element is not None:\n",
    "    next_page_relative_url = next_li_element.find('a', href=True)['href']\n",
    "\n",
    "    # getting the new page\n",
    "    page = requests.get(base_url + next_page_relative_url, headers=headers)\n",
    "\n",
    "    # parsing the new page\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # scraping the new page\n",
    "    scrape_page(soup, quotes)\n",
    "\n",
    "    # looking for the \"Next →\" HTML element in the new page\n",
    "    next_li_element = soup.find('li', class_='next')\n",
    "\n",
    "# reading  the \"quotes.csv\" file and creating it\n",
    "# if not present\n",
    "csv_file = open('quotes.csv', 'w', encoding='utf-8', newline='')\n",
    "\n",
    "# initializing the writer object to insert data\n",
    "# in the CSV file\n",
    "writer = csv.writer(csv_file)\n",
    "\n",
    "# writing the header of the CSV file\n",
    "writer.writerow(['Text', 'Author', 'Tags'])\n",
    "\n",
    "# writing each row of the CSV\n",
    "for quote in quotes:\n",
    "    writer.writerow(quote.values())\n",
    "\n",
    "# terminating the operation and releasing the resources\n",
    "csv_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3755b735588d4303beb95518f2ab4e1c49ea1461d509c805a65d62cd2a8f700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
